# 数据治理 DeepDive Week 2：从脚本到系统 —— 持久化与流式处理

## 🔄 承接上周

上周我们完成了对 CSV 数据的清洗，但那只是一个**内存版**的小工具。如果 CSV 文件变成 **1GB (500 万行)** 怎么办？如果清洗完需要**存入 MySQL** 供业务查询怎么办？
本周任务：**不许重写，重构你的代码**，将其升级为支持数据库写入的工业级应用。

## 🎯 深度目标

1.  **内存防爆（OOM 挑战）**：放弃 `readAllLines()` 这种一次性读取的方式，改用**流式读取 (Stream/Buffer)** 处理超大文件。
2.  **数据落地**：
    - 清洗合格的数据 -> 写入 MySQL `clean_data` 表。
    - 清洗失败的数据 -> 写入 MySQL `error_logs` 表（记录行号、原始内容、失败原因）。
3.  **批量优化**：学会使用 JDBC Batch 或 ORM 的 Batch Insert，拒绝“一条一条插库”。

## 📦 业务场景升级

- **输入**：继续使用之前的“脏数据”CSV（假设现在它膨胀到了 100MB）。
- **处理**：
  - 手机号、日期清洗逻辑保持不变（复用上周逻辑）。
  - **新增**：必须将清洗后的数据持久化到数据库。

## 🤖 AI 提效挑战（必做）

### 1. 数据库设计与 SQL 优化

- **挑战**：设计一张能容纳数百万数据的表。
- **Prompt 示例**：
  > "我需要将清洗后的员工数据存入 MySQL。请帮我设计 `clean_employees` 表结构，考虑到数据量可能有 1000 万，请给出合理的索引建议。同时设计一张 `error_logs` 表，用于存储变长的错误原始数据。"

### 2. 流式处理与内存管理

- **挑战**：如何用极少的内存处理大文件？
- **Prompt 示例 (Java/Node/Python)**：
  > "我有一个 2GB 的 CSV 文件需要读取并清洗。请帮我写一段代码，使用**流式（Stream）**的方式逐行读取，确保内存占用不超过 200MB。不要把整个文件加载到内存里。"

### 3. 批量写入性能

- **挑战**：100 万条数据，如何 10 秒插完？
- **Prompt 示例**：
  > "我现在使用 MyBatis/JPA/Sequelize 插入数据，速度太慢了。请帮我生成一段使用 **Batch Insert (批量插入)** 的代码，每 1000 条提交一次事务，并解释为什么这样比单条插入快。"

## 📅 验收标准 (Definition of Done)

1.  **压力测试**：我们将提供一个重复生成的大 CSV 文件（约 50MB），你的程序必须在不报 `OutOfMemory` 的情况下跑完。
2.  **数据验证**：数据库中 `clean_data` 表的数据量 + `error_logs` 表的数据量 = CSV 总行数。
